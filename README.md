# Language Identification - A statistical approach

**Implementation of relative entropy for language identification** <br>
Author: Anne-Sophie Karmel <br>
Date: 15/04/2020
<br>
## Goal
The goal is to develop an algorithm that detect the language of an arbitrary text (sentences or more lengthy texts). <br>
For example, the algorithm should return “en” for the English sentence “I am currently
eating my breakfast”, or “fr” for the French sentence “J’ai oublié mon parapluie dans l’abribus”.

## Approach

The model used is based on the working paper ["Language identification: Examining the issues"](http://localhost:8888/files/resources/StatisticalLID-II.pdf) by Penelope Sibun and Jeffrey C. Reynar (1996).
<br><br>
The approach to language identification uses relative entropy, also known as Kullback Leibler distance. <br>
The relative entropy between two probability distributions reflects the amount of additional information necessary to encode the second distribution using an optimal code generated for the first distribution. <br>
Practically it is a useful measure of the similarity between probability distributions. Relative entropy ranges from 0 to <img src="https://render.githubusercontent.com/render/math?math=\infty"> with the minimum generated when the two distributions are identical.<br>
<br>
**The equation for the relative entropy:**
<br><br>
<img src="https://render.githubusercontent.com/render/math?math=\displaystyle{D(p \Vert q) = \sum_{x \in \chi} p(x) \cdot log \frac{p(x)}{q(x)}}">
<br><br>

A dataset is provided containing sentences in 21 languages (named “formatted_data”).<br> 
This database is a .csv files whose separator is a “;”. The first column is
the language code, the second one is the text, and the third one is the number of characters in the specified language. <br>
This database is a collection of texts extracted from the European Union Proceedings.
<br>

## Application

To apply relative entropy to language identification, we use a portion of the subset of a coprus associated with a particular language as a training set. The selection is performed randomly. This is done for each of the languages being discriminated.
<br>
For each training set, a probability distribution is generated by first counting particular events, such as character unigrams, bigrams or trigrams. 
<br>
Smoothing ensures that events found in the test data but not in the training data do not cause the relative entropy score to become <img src="https://render.githubusercontent.com/render/math?math=\infty">. Here we choose to apply *Laplace smoothing* (see below).
<br>
These counts are converted to probabilities using the *relative frequency* technique (see below).
<br><br>
Test data, which are constrained to not overlap with the training data, are also selected randomly. For each of the test sets, a probability distribution is generated in the same manner without smoothing step.<br>
<br>
Lastly, every test set is assigned the language which minimizes the relative entropy between the proability distribution of the given test set and the distribution associated with the training set for that language.
<br> Let <img src="https://render.githubusercontent.com/render/math?math=p"> be the distribution associated with the test set <img src="https://render.githubusercontent.com/render/math?math=P"> and <img src="https://render.githubusercontent.com/render/math?math=q_{i}"> be the distribtion associated with training set <img src="https://render.githubusercontent.com/render/math?math=Q_{i}">, whose language is <img src="https://render.githubusercontent.com/render/math?math=L(Q_{i})">. Assume the training sets are numebered 1 to n. <br><br>We assign:
<br>
<br>
<img src="https://render.githubusercontent.com/render/math?math=\displaystyle{L(P)=L(Q_{x})}">
<br><br>where <img src="https://render.githubusercontent.com/render/math?math=x=arg min_{1\leq i \leq n}D(p\Vert q_{i})">
